{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_MNIST_DIGIT_SUM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4iip4Tg9dhzVvkfkvnioI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhiram-ds/git-test/blob/main/NN_MNIST_DIGIT_SUM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PxCwSfZVtvb"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch import Tensor"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IQsM3DNVwbk"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae0nY3M3WQIM",
        "outputId": "32805d4d-9758-4b64-ca10-bab7eaff36da"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input -? OUtput? RF\n",
        "      self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "      self.pool1 = nn.MaxPool2d(2, 2)\n",
        "      self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "      self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "      self.pool2 = nn.MaxPool2d(2, 2)\n",
        "      self.conv5 = nn.Conv2d(256, 512, 3)\n",
        "      self.conv6 = nn.Conv2d(512, 1024, 3)\n",
        "      self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "\n",
        "      # First fully connected layer\n",
        "      self.fc1 = nn.Linear(11, 5, bias=False)\n",
        "      self.fc2 = nn.Linear(5, 1, bias=False)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x1)))))\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        x = self.conv7(x)\n",
        "        x = x.view(-1, 10)\n",
        "\n",
        "        n1 = torch.argmax(x,dim=1).reshape(-1,1)\n",
        "        n2 = torch.cat((n1,x2),dim=1)\n",
        "        #n2 = n2.reshape(-1,11)\n",
        "        n  = self.fc2(self.fc1(n2))\n",
        "\n",
        "        return F.log_softmax(x), n\n",
        "    \n",
        "my_nn = Net()\n",
        "print(my_nn)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv7): Conv2d(1024, 10, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=11, out_features=5, bias=False)\n",
            "  (fc2): Linear(in_features=5, out_features=1, bias=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snRlwIIPWUKK"
      },
      "source": [
        "my_nn = Net().to(device)\n",
        "#summary(my_nn, input_size=((20,),())\n",
        "\n",
        "#summary(my_nn, input_size=[(1, 28, 28), (1,) ])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpFRh3-2_AHa"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "train_data = datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ]))\n",
        "\n",
        "test_data = datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ]))\n",
        "train_random_input = torch.randint(0,10,(60000,))\n",
        "train_target2 = torch.add(train_random_input, train_data.targets)\n",
        "\n",
        "test_random_input = torch.randint(0,10,(10000,))\n",
        "test_target2 = torch.add(test_random_input, test_data.targets)\n",
        "train_one_hot = torch.nn.functional.one_hot(train_random_input)\n",
        "test_one_hot = torch.nn.functional.one_hot(test_random_input)\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(train_data,\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "random_train_dataset = TensorDataset(train_one_hot, train_target2)\n",
        "random_train_loader = torch.utils.data.DataLoader(random_train_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "random_test_dataset = TensorDataset(test_one_hot, test_target2)\n",
        "random_test_loader = torch.utils.data.DataLoader(random_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHNrHASmWvaF"
      },
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "optimizer = torch.optim.SGD(my_nn.parameters(), lr=learning_rate) "
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLWrh1EsW4RN"
      },
      "source": [
        "# Import the tqdm function \n",
        "#from tqdm import tqdm\n",
        "#predicted=torch.FloatTensor().to(device)\n",
        "def train(model, device, train_loader1, train_loader2, optimizer, epoch):\n",
        "    model.train()\n",
        "    #pbar = tqdm(train_loader)\n",
        "    #predicted_sum=torch.FloatTensor().to(device)\n",
        "    for batch_idx, (dt_1, dt_2) in enumerate(zip(train_loader1, train_loader2)):\n",
        "        data1, data2,target1,  target2 = dt_1[0].to(device), dt_2[0].float().to(device), dt_1[1].to(device), dt_2[1].type(torch.LongTensor).to(device)\n",
        "        #print(data1.shape,target1.shape, data2.shape, target2.shape)\n",
        "        optimizer.zero_grad()\n",
        "        output1, output2 = model(data1, data2)\n",
        "        criterion1 = nn.NLLLoss()\n",
        "        loss1 = criterion1(output1, target1)\n",
        "        criterion2 = nn.MSELoss()\n",
        "        loss2 = criterion2(output2, target2.float())\n",
        "        loss = loss1 + loss2\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #predicted_sum=torch.cat((predicted_sum,output2.detach().reshape(-1)))\n",
        "        #pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "        #if batch_idx % 50 == 0:\n",
        "        print(f'loss={loss.item()} batch_id={batch_idx}')\n",
        "    return 0"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX3Q2k7ZXAYf",
        "outputId": "312f793b-67e9-4e3a-c1c5-ef5a1882c8b1"
      },
      "source": [
        "optimizer.zero_grad()\n",
        "for epoch in range(1, 2):\n",
        "    # Train the model using all the defined objects\n",
        "    #predicted=torch.FloatTensor().to(device)\n",
        "    predicted = train(my_nn, device, train_loader, random_train_loader, optimizer, epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=75.78359985351562 batch_id=0\n",
            "loss=82.34078979492188 batch_id=1\n",
            "loss=91.12466430664062 batch_id=2\n",
            "loss=77.95240783691406 batch_id=3\n",
            "loss=92.49732208251953 batch_id=4\n",
            "loss=82.15525817871094 batch_id=5\n",
            "loss=83.57575225830078 batch_id=6\n",
            "loss=92.25463104248047 batch_id=7\n",
            "loss=80.9339828491211 batch_id=8\n",
            "loss=91.98255920410156 batch_id=9\n",
            "loss=94.4958267211914 batch_id=10\n",
            "loss=78.20368957519531 batch_id=11\n",
            "loss=82.64253997802734 batch_id=12\n",
            "loss=92.53591918945312 batch_id=13\n",
            "loss=84.12726593017578 batch_id=14\n",
            "loss=77.38500213623047 batch_id=15\n",
            "loss=83.34376525878906 batch_id=16\n",
            "loss=82.51246643066406 batch_id=17\n",
            "loss=79.1717529296875 batch_id=18\n",
            "loss=79.38369750976562 batch_id=19\n",
            "loss=84.0156478881836 batch_id=20\n",
            "loss=85.23136138916016 batch_id=21\n",
            "loss=72.80091094970703 batch_id=22\n",
            "loss=86.47996520996094 batch_id=23\n",
            "loss=88.51890563964844 batch_id=24\n",
            "loss=88.58478546142578 batch_id=25\n",
            "loss=78.21577453613281 batch_id=26\n",
            "loss=86.19412231445312 batch_id=27\n",
            "loss=81.30707550048828 batch_id=28\n",
            "loss=91.42072296142578 batch_id=29\n",
            "loss=91.5428466796875 batch_id=30\n",
            "loss=83.948486328125 batch_id=31\n",
            "loss=81.95448303222656 batch_id=32\n",
            "loss=79.88101959228516 batch_id=33\n",
            "loss=88.64584350585938 batch_id=34\n",
            "loss=87.32115173339844 batch_id=35\n",
            "loss=75.40078735351562 batch_id=36\n",
            "loss=81.6642074584961 batch_id=37\n",
            "loss=88.89987182617188 batch_id=38\n",
            "loss=78.74488830566406 batch_id=39\n",
            "loss=79.77658081054688 batch_id=40\n",
            "loss=80.68743133544922 batch_id=41\n",
            "loss=82.3464126586914 batch_id=42\n",
            "loss=93.69680786132812 batch_id=43\n",
            "loss=75.30134582519531 batch_id=44\n",
            "loss=88.9758529663086 batch_id=45\n",
            "loss=86.2297134399414 batch_id=46\n",
            "loss=80.84957885742188 batch_id=47\n",
            "loss=85.67388916015625 batch_id=48\n",
            "loss=75.70658874511719 batch_id=49\n",
            "loss=79.48236083984375 batch_id=50\n",
            "loss=77.95757293701172 batch_id=51\n",
            "loss=89.61343383789062 batch_id=52\n",
            "loss=73.4722900390625 batch_id=53\n",
            "loss=82.01652526855469 batch_id=54\n",
            "loss=77.86547088623047 batch_id=55\n",
            "loss=92.30996704101562 batch_id=56\n",
            "loss=82.04019165039062 batch_id=57\n",
            "loss=92.47282409667969 batch_id=58\n",
            "loss=75.86927795410156 batch_id=59\n",
            "loss=78.6371078491211 batch_id=60\n",
            "loss=78.42477416992188 batch_id=61\n",
            "loss=89.16791534423828 batch_id=62\n",
            "loss=80.7658920288086 batch_id=63\n",
            "loss=79.41527557373047 batch_id=64\n",
            "loss=94.74088287353516 batch_id=65\n",
            "loss=86.94548797607422 batch_id=66\n",
            "loss=86.0851058959961 batch_id=67\n",
            "loss=85.70886993408203 batch_id=68\n",
            "loss=71.86747741699219 batch_id=69\n",
            "loss=80.20001220703125 batch_id=70\n",
            "loss=89.026611328125 batch_id=71\n",
            "loss=86.76885223388672 batch_id=72\n",
            "loss=85.67405700683594 batch_id=73\n",
            "loss=82.75069427490234 batch_id=74\n",
            "loss=78.6201400756836 batch_id=75\n",
            "loss=92.55313873291016 batch_id=76\n",
            "loss=79.16340637207031 batch_id=77\n",
            "loss=82.5261001586914 batch_id=78\n",
            "loss=79.78270721435547 batch_id=79\n",
            "loss=76.79219818115234 batch_id=80\n",
            "loss=88.30806732177734 batch_id=81\n",
            "loss=87.10807037353516 batch_id=82\n",
            "loss=79.59795379638672 batch_id=83\n",
            "loss=88.04484558105469 batch_id=84\n",
            "loss=79.9861831665039 batch_id=85\n",
            "loss=80.40955352783203 batch_id=86\n",
            "loss=98.33309936523438 batch_id=87\n",
            "loss=88.94212341308594 batch_id=88\n",
            "loss=84.94584655761719 batch_id=89\n",
            "loss=80.9875717163086 batch_id=90\n",
            "loss=82.4379653930664 batch_id=91\n",
            "loss=82.65267181396484 batch_id=92\n",
            "loss=90.9288558959961 batch_id=93\n",
            "loss=85.06812286376953 batch_id=94\n",
            "loss=84.23839569091797 batch_id=95\n",
            "loss=75.54286193847656 batch_id=96\n",
            "loss=87.11001586914062 batch_id=97\n",
            "loss=79.35859680175781 batch_id=98\n",
            "loss=77.36975860595703 batch_id=99\n",
            "loss=86.54634857177734 batch_id=100\n",
            "loss=97.97122192382812 batch_id=101\n",
            "loss=77.50896453857422 batch_id=102\n",
            "loss=81.10680389404297 batch_id=103\n",
            "loss=86.75790405273438 batch_id=104\n",
            "loss=82.2212905883789 batch_id=105\n",
            "loss=84.77696228027344 batch_id=106\n",
            "loss=78.25208282470703 batch_id=107\n",
            "loss=87.81725311279297 batch_id=108\n",
            "loss=85.62845611572266 batch_id=109\n",
            "loss=79.66155242919922 batch_id=110\n",
            "loss=87.94615173339844 batch_id=111\n",
            "loss=83.96586608886719 batch_id=112\n",
            "loss=73.93169403076172 batch_id=113\n",
            "loss=89.15477752685547 batch_id=114\n",
            "loss=92.23176574707031 batch_id=115\n",
            "loss=79.27349853515625 batch_id=116\n",
            "loss=76.2281494140625 batch_id=117\n",
            "loss=77.62318420410156 batch_id=118\n",
            "loss=92.58126831054688 batch_id=119\n",
            "loss=84.57605743408203 batch_id=120\n",
            "loss=85.61363983154297 batch_id=121\n",
            "loss=90.0060043334961 batch_id=122\n",
            "loss=73.33660888671875 batch_id=123\n",
            "loss=77.47687530517578 batch_id=124\n",
            "loss=84.25321197509766 batch_id=125\n",
            "loss=88.6263198852539 batch_id=126\n",
            "loss=74.57393646240234 batch_id=127\n",
            "loss=80.68132781982422 batch_id=128\n",
            "loss=88.17642211914062 batch_id=129\n",
            "loss=84.75639343261719 batch_id=130\n",
            "loss=102.14529418945312 batch_id=131\n",
            "loss=78.81082153320312 batch_id=132\n",
            "loss=75.35490417480469 batch_id=133\n",
            "loss=89.74696350097656 batch_id=134\n",
            "loss=92.58077239990234 batch_id=135\n",
            "loss=80.76800537109375 batch_id=136\n",
            "loss=88.30918884277344 batch_id=137\n",
            "loss=94.43162536621094 batch_id=138\n",
            "loss=83.93512725830078 batch_id=139\n",
            "loss=82.95101165771484 batch_id=140\n",
            "loss=84.61807250976562 batch_id=141\n",
            "loss=69.00179290771484 batch_id=142\n",
            "loss=86.10494995117188 batch_id=143\n",
            "loss=79.14706420898438 batch_id=144\n",
            "loss=87.67122650146484 batch_id=145\n",
            "loss=81.2965316772461 batch_id=146\n",
            "loss=78.03330993652344 batch_id=147\n",
            "loss=82.17859649658203 batch_id=148\n",
            "loss=73.24263763427734 batch_id=149\n",
            "loss=88.78538513183594 batch_id=150\n",
            "loss=83.45735168457031 batch_id=151\n",
            "loss=78.09014129638672 batch_id=152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "868yfPmNZTvu"
      },
      "source": [
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuD6rAtyZLcg"
      },
      "source": [
        "def test(model, device, test_loader):\n",
        "    test_predict=torch.FloatTensor().to(device)\n",
        "    my_nn.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = my_nn(data)\n",
        "            test_criterion = nn.MSELoss()\n",
        "            test_loss += test_criterion(output, target.float()) # sum up batch loss\n",
        "            #pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            #correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            test_predict=torch.cat((test_predict,output.detach().reshape(-1)))\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f} '.format(test_loss))\n",
        "    return test_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL_Ywob-ze0H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}